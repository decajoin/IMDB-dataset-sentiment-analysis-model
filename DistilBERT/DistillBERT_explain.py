import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from captum.attr import LayerIntegratedGradients
import json
import warnings

warnings.filterwarnings('ignore')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class DistilBERTInterpreter:
    """DistilBERTæ¨¡å‹å¯è§£é‡Šæ€§åˆ†æå™¨"""

    def __init__(self, model_path, config_path, tokenizer_path):
        """
        åˆå§‹åŒ–è§£é‡Šå™¨

        Args:
            model_path: æ¨¡å‹æƒé‡è·¯å¾„ (.pthæ–‡ä»¶)
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ (.jsonæ–‡ä»¶)
            tokenizer_path: tokenizerä¿å­˜è·¯å¾„
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

        # åŠ è½½tokenizer
        self.tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_path)

        # åŠ è½½æ¨¡å‹
        self.model = DistilBertForSequenceClassification.from_pretrained(
            self.config['model_name'],
            num_labels=self.config['num_labels']
        )
        self.model.load_state_dict(torch.load(model_path, map_location=device))
        self.model.to(device)
        self.model.eval()

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! æœ€ä½³å‡†ç¡®ç‡: {self.config['best_accuracy']:.4f}")

    def predict(self, text):
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„æµ‹

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            prediction: é¢„æµ‹ç±»åˆ« (0=negative, 1=positive)
            probabilities: å„ç±»åˆ«çš„æ¦‚ç‡
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=512
        )

        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)

        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)
            prediction = torch.argmax(probs, dim=1).item()

        return prediction, probs.cpu().numpy()[0]

    def integrated_gradients_analysis(self, text, target_class=None):
        """
        ä½¿ç”¨Integrated Gradientsè¿›è¡Œå½’å› åˆ†æ

        Args:
            text: è¾“å…¥æ–‡æœ¬
            target_class: ç›®æ ‡ç±»åˆ«(Noneåˆ™ä½¿ç”¨é¢„æµ‹ç±»åˆ«)

        Returns:
            tokens: tokenåˆ—è¡¨
            attributions: å„tokençš„å½’å› åˆ†æ•°
        """
        # é¢„å¤„ç†
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=512
        )

        input_ids = inputs['input_ids'].to(device)
        attention_mask = inputs['attention_mask'].to(device)

        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ç±»åˆ«ï¼Œä½¿ç”¨é¢„æµ‹ç±»åˆ«
        if target_class is None:
            with torch.no_grad():
                outputs = self.model(input_ids, attention_mask=attention_mask)
                target_class = torch.argmax(outputs.logits, dim=1).item()

        # å®šä¹‰å‰å‘ä¼ æ’­å‡½æ•°
        def forward_func(input_ids, attention_mask):
            outputs = self.model(input_ids, attention_mask=attention_mask)
            return outputs.logits

        # åˆ›å»ºIntegrated Gradientså¯¹è±¡
        lig = LayerIntegratedGradients(forward_func, self.model.distilbert.embeddings)

        # è®¡ç®—å½’å› 
        attributions, delta = lig.attribute(
            inputs=(input_ids, attention_mask),
            target=target_class,
            n_steps=50,
            return_convergence_delta=True,
            additional_forward_args=None
        )

        # å¯¹embeddingç»´åº¦æ±‚å’Œ
        attributions = attributions.sum(dim=-1).squeeze(0)
        attributions = attributions / torch.norm(attributions)
        attributions = attributions.cpu().detach().numpy()

        # è·å–tokens
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])

        return tokens, attributions

    def visualize_importance(self, text, save_path=None):
        """
        å¯è§†åŒ–è¯é‡è¦æ€§ï¼ˆçƒ­åŠ›å›¾+æŸ±çŠ¶å›¾ç»„åˆï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬
            save_path: ä¿å­˜è·¯å¾„
        """
        # è·å–é¢„æµ‹å’Œå½’å› åˆ†æ•°
        prediction, probs = self.predict(text)
        tokens, attributions = self.integrated_gradients_analysis(text)

        # è¿‡æ»¤padding tokens
        valid_pairs = [(tok, attr) for tok, attr in zip(tokens, attributions)
                       if tok != '[PAD]']

        if len(valid_pairs) > 60:
            valid_pairs = valid_pairs[:60]

        tokens = [pair[0] for pair in valid_pairs]
        attributions = np.array([pair[1] for pair in valid_pairs])

        # æ ‡å‡†åŒ–é‡è¦æ€§åˆ†æ•°åˆ°[0, 1]
        attr_min, attr_max = attributions.min(), attributions.max()
        if attr_max - attr_min > 1e-8:
            normalized_scores = (attributions - attr_min) / (attr_max - attr_min)
        else:
            normalized_scores = np.zeros_like(attributions)

        # åˆ›å»ºå¯è§†åŒ–
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10))

        # ========== 1. è¯é‡è¦æ€§çƒ­åŠ›å›¾ ==========
        colors = ['blue', 'white', 'red']
        cmap = LinearSegmentedColormap.from_list('importance', colors, N=256)

        importance_matrix = normalized_scores.reshape(1, -1)
        im = ax1.imshow(importance_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)

        # è®¾ç½®xè½´æ ‡ç­¾
        ax1.set_xticks(range(len(tokens)))
        ax1.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)
        ax1.set_yticks([])

        sentiment = "Positive" if prediction == 1 else "Negative"
        confidence = probs[prediction]
        ax1.set_title(
            f'Token Importance Heatmap\n'
            f'Prediction: {sentiment} (Confidence: {confidence:.3f})',
            fontsize=12, fontweight='bold', pad=15
        )

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax1, orientation='horizontal', pad=0.15, shrink=0.8)
        cbar.set_label('Normalized Importance Score', fontsize=10)

        # ========== 2. é‡è¦æ€§åˆ†æ•°æŸ±çŠ¶å›¾ ==========
        bars = ax2.bar(range(len(tokens)), attributions.tolist(), alpha=0.7)

        # æ ¹æ®é‡è¦æ€§ç»™æŸ±å­ç€è‰²
        for i, bar in enumerate(bars):
            if attributions[i] > 0:
                bar.set_color('red')
                bar.set_alpha(0.7)
            else:
                bar.set_color('blue')
                bar.set_alpha(0.7)

        ax2.set_xticks(range(len(tokens)))
        ax2.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)
        ax2.set_ylabel('Importance Score', fontsize=10)
        ax2.set_title('Token Importance Scores',
                      fontsize=12, fontweight='bold', pad=10)
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=1.5, alpha=0.5)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å¯è§†åŒ–å›¾å·²ä¿å­˜åˆ°: {save_path}")

        plt.show()

        # æ‰“å°è¯¦ç»†åˆ†æç»“æœ
        self.print_analysis_results(text, tokens, attributions, prediction, probs)

    def print_analysis_results(self, text, tokens, attributions, prediction, probs):
        """æ‰“å°è¯¦ç»†åˆ†æç»“æœ"""
        print("\n" + "=" * 80)
        print("DistilBERT æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æç»“æœ")
        print("=" * 80)
        print(f"è¾“å…¥æ–‡æœ¬: {text}")
        print(f"é¢„æµ‹ç»“æœ: {'æ­£é¢æƒ…æ„Ÿ ğŸ˜Š' if prediction == 1 else 'è´Ÿé¢æƒ…æ„Ÿ ğŸ˜'}")
        print(f"ç½®ä¿¡åº¦: {probs[prediction]:.4f}")
        print(f"æ¦‚ç‡åˆ†å¸ƒ: è´Ÿé¢={probs[0]:.4f}, æ­£é¢={probs[1]:.4f}")
        print(f"åˆ†ææ–¹æ³•: ç§¯åˆ†æ¢¯åº¦æ³•")
        print("-" * 80)

        # æ‰¾å‡ºæœ€é‡è¦çš„è¯
        token_attr_pairs = [(tok, attr) for tok, attr in zip(tokens, attributions)
                            if tok not in ['[CLS]', '[SEP]', '[PAD]']]
        token_attr_pairs.sort(key=lambda x: abs(x[1]), reverse=True)

        print("æœ€é‡è¦çš„è¯æ±‡ (æŒ‰é‡è¦æ€§æ’åº):")
        print("-" * 60)
        for i, (token, score) in enumerate(token_attr_pairs[:15], 1):
            direction = "æ­£å‘" if score > 0 else "è´Ÿå‘"
            print(f"{i:2d}. {token:15s} {score:+8.4f}  ({direction})")

        print("-" * 80)

        # ç»Ÿè®¡ä¿¡æ¯
        positive_attrs = [attr for attr in attributions if attr > 0]
        negative_attrs = [attr for attr in attributions if attr < 0]

        print("ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   â€¢ æ€»tokenæ•°é‡: {len(tokens)}")
        print(f"   â€¢ æ­£å‘è´¡çŒ®è¯æ±‡: {len(positive_attrs)} ä¸ª")
        print(f"   â€¢ è´Ÿå‘è´¡çŒ®è¯æ±‡: {len(negative_attrs)} ä¸ª")
        if positive_attrs:
            print(f"   â€¢ æœ€å¤§æ­£å‘å½’å› : {max(positive_attrs):.4f}")
        if negative_attrs:
            print(f"   â€¢ æœ€å¤§è´Ÿå‘å½’å› : {min(negative_attrs):.4f}")
        print("=" * 80 + "\n")


def main():
    """ä¸»å‡½æ•°ï¼šäº¤äº’å¼å¯è§£é‡Šæ€§åˆ†æ"""

    print("=" * 80)
    print("DistilBERT æƒ…æ„Ÿåˆ†æ - å¯è§£é‡Šæ€§å·¥å…·")
    print("=" * 80)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print()

    # åˆå§‹åŒ–è§£é‡Šå™¨
    try:
        interpreter = DistilBERTInterpreter(
            model_path="distilbert_imdb_best.pth",
            config_path="distilbert_imdb_best_config.json",
            tokenizer_path="distilbert_imdb_best_tokenizer"
        )
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶")
        print(f"   {e}")
        return

    # é¢„è®¾æµ‹è¯•è¯„è®º
    sample_reviews = [
        "This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.",
        "Terrible film. Waste of time and money. I've never been so bored in my life.",
        "The movie had some good moments, but overall it was just okay. Nothing special.",
        "One of the best films I've ever seen! Brilliant performances and an incredible storyline.",
        "I really hate this film. It's boring and the plot makes no sense at all.",
    ]

    while True:
        print("\n" + "=" * 60)
        print("è¯·é€‰æ‹©æ“ä½œ:")
        print("=" * 60)
        print("1. åˆ†æé¢„è®¾è¯„è®º")
        print("2. è¾“å…¥è‡ªå®šä¹‰è¯„è®º")
        print("3. æ‰¹é‡åˆ†æé¢„è®¾è¯„è®º")
        print("4. é€€å‡º")
        print("=" * 60)

        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()

        if choice == '1':
            print("\né¢„è®¾è¯„è®ºåˆ—è¡¨:")
            print("-" * 60)
            for i, review in enumerate(sample_reviews, 1):
                print(f"{i}. {review}")
            print("-" * 60)

            try:
                idx = int(input(f"\nè¯·é€‰æ‹©è¯„è®º (1-{len(sample_reviews)}): ")) - 1
                if 0 <= idx < len(sample_reviews):
                    selected_review = sample_reviews[idx]

                    print(f"\næ­£åœ¨åˆ†æè¯„è®º...")
                    print(f"{selected_review}")
                    print("-" * 60)

                    interpreter.visualize_importance(selected_review)
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
            except Exception as e:
                print(f"âŒ åˆ†æå‡ºé”™: {e}")

        elif choice == '2':
            custom_review = input("\nè¯·è¾“å…¥æ‚¨çš„è¯„è®º: ").strip()
            if custom_review:
                print(f"\næ­£åœ¨åˆ†ææ‚¨çš„è¯„è®º...")
                print(f"{custom_review}")
                print("-" * 60)

                try:
                    interpreter.visualize_importance(custom_review)
                except Exception as e:
                    print(f"âŒ åˆ†æå‡ºé”™: {e}")
            else:
                print("âŒ è¯„è®ºä¸èƒ½ä¸ºç©º")

        elif choice == '3':
            print(f"\nğŸ” æ‰¹é‡åˆ†æ {len(sample_reviews)} æ¡é¢„è®¾è¯„è®º...")
            print("-" * 60)

            for i, review in enumerate(sample_reviews, 1):
                print(f"\n[{i}/{len(sample_reviews)}] åˆ†æä¸­...")
                print(f"{review[:80]}..." if len(review) > 80 else f"{review}")

                try:
                    interpreter.visualize_importance(review)
                except Exception as e:
                    print(f"âŒ åˆ†æå‡ºé”™: {e}")
                    continue

            print("\nâœ… æ‰¹é‡åˆ†æå®Œæˆ!")

        elif choice == '4':
            print("\nå†è§ï¼")
            break

        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-4")


if __name__ == "__main__":
    main()