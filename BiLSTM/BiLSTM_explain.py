import torch
import torch.nn as nn
import numpy as np
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
import warnings
from BiLSTM_model import BiLSTM

warnings.filterwarnings('ignore')

# ----------------------------
# å¯è§£é‡Šæ€§åˆ†æžç±»
# ----------------------------
class BiLSTMInterpreter:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.model.eval()

    def preprocess_text(self, text, max_length=512):
        """æ–‡æœ¬é¢„å¤„ç†"""
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors='pt'
        )
        return encoding['input_ids']

    def get_token_importance_occlusion(self, text, target_class=None):
        """ä½¿ç”¨é®æŒ¡æ³•è®¡ç®—æ¯ä¸ªtokençš„é‡è¦æ€§"""
        input_ids = self.preprocess_text(text).to(self.device)

        # åŽŸå§‹é¢„æµ‹
        with torch.no_grad():
            logits = self.model(input_ids)
            probs = torch.softmax(logits, dim=1)

        if target_class is None:
            target_class = probs.argmax(dim=1).item()

        original_score = probs[0, target_class].item()

        importance_scores = []
        tokens = input_ids[0].cpu().numpy()

        for i in range(len(tokens)):
            token_id = tokens[i]
            # å¿½ç•¥ç‰¹æ®Šç¬¦å·
            if token_id in [
                self.tokenizer.pad_token_id,
                self.tokenizer.cls_token_id,
                self.tokenizer.sep_token_id
            ]:
                importance_scores.append(0.0)
                continue

            # é®æŒ¡è¯¥tokenï¼ˆç”¨UNK tokenæ›¿æ¢ï¼‰
            masked_ids = input_ids.clone()
            masked_ids[0, i] = self.tokenizer.unk_token_id

            with torch.no_grad():
                masked_logits = self.model(masked_ids)
                masked_probs = torch.softmax(masked_logits, dim=1)
                masked_score = masked_probs[0, target_class].item()

            importance = original_score - masked_score
            importance_scores.append(importance)

        return np.array(importance_scores), target_class, original_score, probs[0].cpu().numpy()

    def visualize_importance(self, text, save_path=None):
        """ç»˜åˆ¶è¯é‡è¦æ€§å¯è§†åŒ–"""
        importance_scores, predicted_class, confidence, probs = self.get_token_importance_occlusion(text)

        # èŽ·å–tokens
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=512)
        tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'])

        # è¿‡æ»¤paddingéƒ¨åˆ†
        valid_indices = [i for i, t in enumerate(tokens) if t != self.tokenizer.pad_token]
        tokens = [tokens[i] for i in valid_indices]
        importance_scores = importance_scores[valid_indices]

        # æ ‡å‡†åŒ–
        norm_scores = (importance_scores - importance_scores.min()) / (
            importance_scores.max() - importance_scores.min() + 1e-8
        )

        # è‡ªå®šä¹‰é¢œè‰²æ˜ å°„
        cmap = LinearSegmentedColormap.from_list('importance', ['blue', 'white', 'red'], N=256)

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))

        # (1) çƒ­åŠ›å›¾
        im = ax1.imshow(norm_scores.reshape(1, -1), cmap=cmap, aspect='auto', vmin=0, vmax=1)
        ax1.set_xticks(range(len(tokens)))
        ax1.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)
        ax1.set_yticks([])
        sentiment = "Positive ðŸ˜Š" if predicted_class == 1 else "Negative ðŸ˜ž"
        ax1.set_title(f"Token Importance Heatmap\nPrediction: {sentiment} (Confidence: {confidence:.3f})",
                      fontsize=12, fontweight='bold')
        plt.colorbar(im, ax=ax1, orientation='horizontal', pad=0.1, shrink=0.8, label='Normalized Importance')

        # (2) æŸ±çŠ¶å›¾
        bars = ax2.bar(range(len(tokens)), importance_scores, color='gray', alpha=0.7)
        for i, bar in enumerate(bars):
            if importance_scores[i] > 0:
                bar.set_color('red')
            else:
                bar.set_color('blue')
        ax2.set_xticks(range(len(tokens)))
        ax2.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)
        ax2.set_ylabel('Importance Score')
        ax2.set_title('Token Importance (Occlusion Method)')
        ax2.axhline(0, color='black', linewidth=0.8)
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        self.print_analysis_results(text, tokens, importance_scores, predicted_class, confidence, probs)

    def print_analysis_results(self, text, tokens, importance_scores, predicted_class, confidence, probs):
        """æ‰“å°è¯¦ç»†åˆ†æžç»“æžœ"""
        print("=" * 80)
        print("BiLSTM æ¨¡åž‹å¯è§£é‡Šæ€§åˆ†æžç»“æžœ")
        print("=" * 80)
        print(f"è¾“å…¥æ–‡æœ¬: {text}")
        print(f"é¢„æµ‹ç»“æžœ: {'æ­£é¢æƒ…æ„Ÿ ðŸ˜Š' if predicted_class == 1 else 'è´Ÿé¢æƒ…æ„Ÿ ðŸ˜ž'}")
        print(f"ç½®ä¿¡åº¦: {confidence:.3f}")
        print(f"æ¦‚çŽ‡åˆ†å¸ƒ: è´Ÿé¢={probs[0]:.3f}, æ­£é¢={probs[1]:.3f}")
        print(f"åˆ†æžæ–¹æ³•: é®æŒ¡æ³• (Occlusion)")
        print("-" * 80)

        token_pairs = list(zip(tokens, importance_scores))
        token_pairs.sort(key=lambda x: abs(x[1]), reverse=True)
        print("æœ€é‡è¦çš„è¯æ±‡ (æŒ‰é‡è¦æ€§æŽ’åº):")
        print("-" * 40)
        for i, (token, score) in enumerate(token_pairs[:10]):
            if token not in ['[CLS]', '[SEP]', '[PAD]']:
                print(f"{i + 1:2d}. {token:15s} {score:8.4f}")

        print("-" * 80)
        pos_scores = [s for s in importance_scores if s > 0]
        neg_scores = [s for s in importance_scores if s < 0]
        print(f"æ€»tokenæ•°: {len(tokens)}")
        print(f"   â€¢ æ­£å‘é‡è¦è¯: {len(pos_scores)} ä¸ª")
        print(f"   â€¢ è´Ÿå‘é‡è¦è¯: {len(neg_scores)} ä¸ª")
        if pos_scores:
            print(f"   â€¢ æœ€å¤§æ­£å‘é‡è¦æ€§: {max(pos_scores):.4f}")
        if neg_scores:
            print(f"   â€¢ æœ€å¤§è´Ÿå‘é‡è¦æ€§: {min(neg_scores):.4f}")
        print("=" * 80)


# ----------------------------
# ä¸»å‡½æ•°
# ----------------------------
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

    model = BiLSTM(
        vocab_size=tokenizer.vocab_size,
        embed_size=128,
        hidden_size=128,
        num_layers=2,
        num_classes=2,
        dropout=0.5,
        bidirectional=True
    ).to(device)

    # åŠ è½½æ¨¡åž‹æƒé‡
    try:
        model.load_state_dict(torch.load('bilstm_imdb_best.pth', map_location=device))
        print("âœ… æˆåŠŸåŠ è½½è®­ç»ƒå¥½çš„ BiLSTM æ¨¡åž‹")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æ¨¡åž‹æ–‡ä»¶ 'bilstm_imdb_best.pth'")
        return

    interpreter = BiLSTMInterpreter(model, tokenizer, device)

    # ç¤ºä¾‹è¯„è®º
    sample_reviews = [
        "One of the best movies I've ever seen! Brilliant acting and incredible plot.",
        "I really hate this film. It's boring and the plot makes no sense at all.",
        "The movie was okay, nothing special but not terrible either. Average experience."
    ]

    print("\n" + "=" * 60)
    print("BiLSTM ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æž - å¯è§£é‡Šæ€§å·¥å…·")
    print("=" * 60)

    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. åˆ†æžé¢„è®¾è¯„è®º")
        print("2. è¾“å…¥è‡ªå®šä¹‰è¯„è®º")
        print("3. é€€å‡º")

        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()

        if choice == '1':
            print("\né¢„è®¾è¯„è®º:")
            for i, review in enumerate(sample_reviews, 1):
                print(f"{i}. {review}")

            try:
                idx = int(input(f"\nè¯·é€‰æ‹©è¯„è®º (1-{len(sample_reviews)}): ")) - 1
                if 0 <= idx < len(sample_reviews):
                    print(f"\næ­£åœ¨åˆ†æžè¯„è®º: {sample_reviews[idx]}")
                    interpreter.visualize_importance(sample_reviews[idx])
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

        elif choice == '2':
            text = input("\nè¯·è¾“å…¥è‡ªå®šä¹‰è¯„è®º: ").strip()
            if text:
                print(f"\næ­£åœ¨åˆ†æžè¯„è®º: {text}")
                interpreter.visualize_importance(text)
            else:
                print("âŒ è¯„è®ºä¸èƒ½ä¸ºç©º")

        elif choice == '3':
            print("\nå†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


if __name__ == "__main__":
    main()
