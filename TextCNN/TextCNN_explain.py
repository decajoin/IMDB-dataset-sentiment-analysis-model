import torch
import torch.nn as nn
import numpy as np
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap
import warnings
from  TextCNN_model import TextCNN

warnings.filterwarnings('ignore')

# ----------------------------
# å¯è§£é‡Šæ€§åˆ†æžç±»
# ----------------------------
class TextCNNInterpreter:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.model.eval()

    def preprocess_text(self, text, max_length=512):
        """æ–‡æœ¬é¢„å¤„ç†"""
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors='pt'
        )
        return encoding['input_ids']

    def get_token_importance_occlusion(self, text, target_class=None):
        """ä½¿ç”¨é®æŒ¡æ–¹æ³•è®¡ç®—æ¯ä¸ªtokençš„é‡è¦æ€§"""
        input_ids = self.preprocess_text(text).to(self.device)

        # èŽ·å–åŽŸå§‹é¢„æµ‹
        with torch.no_grad():
            original_output = self.model(input_ids)
            original_prob = torch.softmax(original_output, dim=1)

        if target_class is None:
            target_class = original_output.argmax(dim=1).item()

        original_score = original_prob[0, target_class].item()

        # è®¡ç®—æ¯ä¸ªä½ç½®çš„é‡è¦æ€§
        importance_scores = []
        tokens = input_ids[0].cpu().numpy()

        for i in range(len(tokens)):
            if tokens[i] in [self.tokenizer.pad_token_id, self.tokenizer.cls_token_id, self.tokenizer.sep_token_id]:
                importance_scores.append(0.0)
                continue

            # åˆ›å»ºé®æŒ¡ç‰ˆæœ¬ï¼ˆç”¨UNK tokenæ›¿æ¢ï¼‰
            masked_ids = input_ids.clone()
            masked_ids[0, i] = self.tokenizer.unk_token_id

            with torch.no_grad():
                masked_output = self.model(masked_ids)
                masked_prob = torch.softmax(masked_output, dim=1)
                masked_score = masked_prob[0, target_class].item()

            # é‡è¦æ€§ = åŽŸå§‹åˆ†æ•° - é®æŒ¡åŽåˆ†æ•°
            importance = original_score - masked_score
            importance_scores.append(importance)

        return np.array(importance_scores), target_class, original_score, original_prob[0].cpu().numpy()

    def visualize_importance(self, text, save_path=None):
        """å¯è§†åŒ–è¯é‡è¦æ€§"""
        importance_scores, predicted_class, confidence, probs = self.get_token_importance_occlusion(text)

        # èŽ·å–tokens
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=512)
        tokens = self.tokenizer.convert_ids_to_tokens(encoding['input_ids'])

        # è¿‡æ»¤padding tokens
        valid_indices = [i for i, token in enumerate(tokens) if token != self.tokenizer.pad_token]
        tokens = [tokens[i] for i in valid_indices]
        importance_scores = importance_scores[valid_indices]

        # åˆ›å»ºå¯è§†åŒ–
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))

        # 1. è¯é‡è¦æ€§çƒ­åŠ›å›¾
        # æ ‡å‡†åŒ–é‡è¦æ€§åˆ†æ•°
        normalized_scores = (importance_scores - importance_scores.min()) / (
                    importance_scores.max() - importance_scores.min() + 1e-8)

        # åˆ›å»ºè‡ªå®šä¹‰é¢œè‰²æ˜ å°„
        colors = ['blue', 'white', 'red']
        cmap = LinearSegmentedColormap.from_list('importance', colors, N=256)

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        importance_matrix = normalized_scores.reshape(1, -1)
        im = ax1.imshow(importance_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)

        # è®¾ç½®xè½´æ ‡ç­¾
        ax1.set_xticks(range(len(tokens)))
        ax1.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)
        ax1.set_yticks([])
        ax1.set_title(
            f'Token Importance Heatmap \nPrediction: {"Positive" if predicted_class == 1 else "Negative"} (Confidence: {confidence:.3f})',
            fontsize=12, fontweight='bold')

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax1, orientation='horizontal', pad=0.1, shrink=0.8)
        cbar.set_label('Normalized Importance Score', fontsize=10)

        # 2. é‡è¦æ€§åˆ†æ•°æŸ±çŠ¶å›¾
        bars = ax2.bar(range(len(tokens)), importance_scores, alpha=0.7)

        # æ ¹æ®é‡è¦æ€§ç»™æŸ±å­ç€è‰²
        for i, bar in enumerate(bars):
            if importance_scores[i] > 0:
                bar.set_color('red')
                bar.set_alpha(0.7)
            else:
                bar.set_color('blue')
                bar.set_alpha(0.7)

        ax2.set_xticks(range(len(tokens)))
        ax2.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)
        ax2.set_ylabel('Importance Score', fontsize=10)
        ax2.set_title('Token Importance Scores', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

        # æ‰“å°è¯¦ç»†åˆ†æžç»“æžœ
        self.print_analysis_results(text, tokens, importance_scores, predicted_class, confidence, probs)

    def print_analysis_results(self, text, tokens, importance_scores, predicted_class, confidence, probs):
        """æ‰“å°è¯¦ç»†åˆ†æžç»“æžœ"""
        print("=" * 80)
        print("TextCNN æ¨¡åž‹å¯è§£é‡Šæ€§åˆ†æžç»“æžœ")
        print("=" * 80)
        print(f"è¾“å…¥æ–‡æœ¬: {text}")
        print(f"é¢„æµ‹ç»“æžœ: {'æ­£é¢æƒ…æ„Ÿ ðŸ˜Š' if predicted_class == 1 else 'è´Ÿé¢æƒ…æ„Ÿ ðŸ˜ž'}")
        print(f"ç½®ä¿¡åº¦: {confidence:.3f}")
        print(f"æ¦‚çŽ‡åˆ†å¸ƒ: è´Ÿé¢={probs[0]:.3f}, æ­£é¢={probs[1]:.3f}")
        print(f"åˆ†æžæ–¹æ³•: é®æŒ¡æ³• (Occlusion)")
        print("-" * 80)

        # æ‰¾å‡ºæœ€é‡è¦çš„è¯
        token_importance_pairs = list(zip(tokens, importance_scores))
        token_importance_pairs.sort(key=lambda x: abs(x[1]), reverse=True)

        print("æœ€é‡è¦çš„è¯æ±‡ (æŒ‰é‡è¦æ€§æŽ’åº):")
        print("-" * 40)
        for i, (token, score) in enumerate(token_importance_pairs[:10]):
            if token not in ['[CLS]', '[SEP]', '[PAD]']:
                print(f"{i + 1:2d}. {token:15s} {score:8.4f}")

        print("-" * 80)

        # ç»Ÿè®¡ä¿¡æ¯
        positive_importance = [score for score in importance_scores if score > 0]
        negative_importance = [score for score in importance_scores if score < 0]

        print("ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   â€¢ æ€»tokenæ•°é‡: {len(tokens)}")
        print(f"   â€¢ æ­£å‘é‡è¦æ€§è¯æ±‡: {len(positive_importance)} ä¸ª")
        print(f"   â€¢ è´Ÿå‘é‡è¦æ€§è¯æ±‡: {len(negative_importance)} ä¸ª")
        if positive_importance:
            print(f"   â€¢ æœ€å¤§æ­£å‘é‡è¦æ€§: {max(positive_importance):.4f}")
        if negative_importance:
            print(f"   â€¢ æœ€å¤§è´Ÿå‘é‡è¦æ€§: {min(negative_importance):.4f}")
        print("=" * 80)


# ----------------------------
# ä¸»å‡½æ•°
# ----------------------------
def main():
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

    # åˆå§‹åŒ–æ¨¡åž‹
    model = TextCNN(
        vocab_size=tokenizer.vocab_size,
        embed_size=128,
        num_filters=100,
        filter_sizes=[3, 4, 5],
        num_classes=2,
        dropout=0.5
    ).to(device)

    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹æƒé‡
    try:
        model.load_state_dict(torch.load('textcnn_imdb_best.pth', map_location=device))
        print("âœ… æˆåŠŸåŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æ¨¡åž‹æ–‡ä»¶ 'textcnn_imdb_best.pth'")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡åž‹æ–‡ä»¶")
        return

    # åˆ›å»ºè§£é‡Šå™¨
    interpreter = TextCNNInterpreter(model, tokenizer, device)

    # é¢„è®¾æµ‹è¯•è¯„è®º
    sample_reviews = [
        "One of the best movies I've ever seen! Brilliant acting and incredible plot.",
        "I really hate this film. It's boring and the plot makes no sense at all.",
        "The movie was okay, nothing special but not terrible either. Average experience."
    ]

    print("\n" + "=" * 60)
    print("TextCNN ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æž - å¯è§£é‡Šæ€§å·¥å…·")
    print("=" * 60)

    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. åˆ†æžé¢„è®¾è¯„è®º")
        print("2. è¾“å…¥è‡ªå®šä¹‰è¯„è®º")
        print("3. é€€å‡º")

        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()

        if choice == '1':
            print("\né¢„è®¾è¯„è®ºåˆ—è¡¨:")
            for i, review in enumerate(sample_reviews, 1):
                print(f"{i}. {review}")

            try:
                idx = int(input(f"\nè¯·é€‰æ‹©è¯„è®º (1-{len(sample_reviews)}): ")) - 1
                if 0 <= idx < len(sample_reviews):
                    selected_review = sample_reviews[idx]

                    print(f"\næ­£åœ¨åˆ†æžè¯„è®º: {selected_review}")
                    print("ä½¿ç”¨æ–¹æ³•: é®æŒ¡æ³• (Occlusion)")
                    print("-" * 60)

                    interpreter.visualize_importance(selected_review)
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

        elif choice == '2':
            custom_review = input("\nè¯·è¾“å…¥æ‚¨çš„è¯„è®º: ").strip()
            if custom_review:
                print(f"\næ­£åœ¨åˆ†æžæ‚¨çš„è¯„è®º: {custom_review}")
                print("ä½¿ç”¨æ–¹æ³•: é®æŒ¡æ³• (Occlusion)")
                print("-" * 60)

                interpreter.visualize_importance(custom_review)
            else:
                print("âŒ è¯„è®ºä¸èƒ½ä¸ºç©º")

        elif choice == '3':
            print("\nå†è§ï¼")
            break

        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


if __name__ == "__main__":
    main()